---
title: "Curating a modern dataset for evaluating cache eviction algorithms"
layout: post
date:   2028-12-28 00:00:00 -0400
author: Juncheng
category: [system, cache]
use_math: true
---

## Introduction
Caching is a widely used technique to improve the performance of many systems. A cache is a small, fast memory that stores a subset of data from a larger, slower storage system. 
Caches are used in a variety of systems, from the browser caches in personal devices, CDN caches on the edge, to various key-value and page caches inside the cloud data centers. 

An important problem in cache management is cache eviction, which is the process of selecting an object to evict from the cache when the cache is full. The evaluation of cache eviction algorithms often relies on real-world traces. 


## The need for a new dataset
Across the literature, there are many different datasets used for evaluating cache eviction algorithms. And it is often the case that different algoritms use different datasets and claim to be superior than previous works. It is unclear whether the superiority is due to the algorithm itself or the dataset used. 

Moreover, most traces do not come with a clear description of the system generating the traces and how the traces were collected. 
For example, whether the traces were collected after a page cache, or whether there is an application layer cache in front of the key-value cache. This information is important. For example, if you are evaluating or developing algorithms for kernel page cache, then you should not use thoses traces filtered by page cache. If you evaluate algorithms for a distributed key-value cache, then you should not use the traces collected from a personal laptop[1]. 

In addition, many widely used traces are outdated and do not reflect the modern workloads. For example, the traces used in [Caffeine](https://github.com/ben-manes/caffeine/wiki/Efficiency) and [Ristretto](https://github.com/dgraph-io/benchmarks/tree/master/cachebench/ristretto/trace) were collected more than 20 years ago; and the popular [Microsoft Cambridge](http://iotta.snia.org/traces/block-io/388) Traces were also close to 20 years old. 
Over the past two decades, cache use cases and application behavior have changed significantly. The most notable difference is increasing deployments of distributed caches and key-value caches. Even for the local caches, the access patterns may also have changed. For example, most local caches, e.g., the page cache of a storage server, are filtered by multiple layers of caches, e.g., application caches and distributed key-value caches. 

Therefore, there is a need for a new dataset for evaluating cache eviction algorithms.

## New datasets open-sourced by industry
As collecting traces from production has become infinitely difficult for researchers these days, multiple companies have released different types of cache traces for research purposes. 
I will describe each dataset briefly below. 

### Twitter (key-value traces)
This dataset was released by Twitter and CMU in the [OSDI'20 paper](https://www.usenix.org/conference/osdi20/presentation/yang). Information about the traces can be found on [GitHub](https://github.com/twitter/cache-trace). The traces can be downloaded from [CMU website](https://ftp.pdl.cmu.edu/pub/datasets/twemcacheWorkload/open_source/). 

The dataset has 54 week-long traces collected from 54 different production twemcache clusters in 2020. 
Twemcache is a fork of Memcached, therefore, it is a lookaside key-value cache. 
There are in total 700 billion requests. This dataset is huge because _most_ of these cache clusters are first-layer caches (there are hot object caches in some microservices/clients). 
Despite the large number of traces, the authors[2] recommended cluster52 for evaluating cache eviction algorithms. 

#### More information
Due to the large number of traces, I will not list the statistics here. Interested readers can find more information in the [OSDI'20 paper](https://www.usenix.org/conference/osdi20/presentation/yang). 
Note that because the caches are lookaside caches, there are many GET requests with size 0. These requests are get misses, and are often followed by a immediate SET requests. This can also be seen on the key-value traces from meta dataset. 


<!-- This is the **only dataset** that 
* was collected from first-layer caches
* was collected from single-tenanted caches, i.e., each cache cluster is dedicated to one microservice. However, each microservice may store multiple types of data in the same cache
* preserves the key structure of the requests 
* contains TTLs of the write requests -->


**My complaints**
* Note that the statistics of the traces on GitHub are incorrect. The correct statistics can be found upon contacting the authors.
* Most of the traces have a very low miss ratio, which means a tiny cache size, e.g., 10 MB is sufficient to achieve compulsory miss ratio. Such traces are not useful for cache eviction algorithm evaluation. 

**Caveats**


### Meta (CDN traces and key-value traces)
Meta released two datasets in 2022 and 2023, the details and download link can be found [here](https://cachelib.org/docs/Cache_Library_User_Guides/Cachebench_FB_HW_eval#list-of-traces).

#### Key-value cache dataset
The first dataset was collected in 2022 and contains one five-day trace from key-value cache clusters. 
The dataset contains around 1.6 billion requests (958 GB) for 52 million objects (60 GB). 
The compulsory request miss ratio is around 3.2% and the compulsory byte miss ratio is around 6.3%.
The mean object size is 624 bytes, the mean request size is around 1240 bytes. The mean access frequency is around 32. 

While there is little information about the cache generating the traces, Facebook has multiple papers about their multi-tenanted key-value caches, e.g., [workload analysis on Sigmetrics'12](https://dl.acm.org/doi/10.1145/2254756.2254766), [Memcached optimization at NSDI'14](https://www.usenix.org/system/files/conference/nsdi13/nsdi13-final170_update.pdf), and [Cachelib at OSDI'20](https://www.usenix.org/conference/osdi20/presentation/berg).


#### CDN dataset
The second dataset was collected in 2023 and contains three week-long traces from different CDN clusters. 
The traces `rnha, rprn, reag` contain 96,88,45 million requests (4182 TB, 3446 TB, 1172 TB) for 35,29,12 million objects (712 TB, 682 TB, 168 TB). 
The mean object size are 46 MB, 42 MB, 28 MB, and the mean request size are 22 MB, 42 MB and 15 MB each. 
The compulsory request miss ratios are 0.36, 0.33, 0.27, and the compulsory byte miss ratios are 17%, 20% and 15%. 
The mean object frequency are 2.76, 3.04, 3.72, respectively. 

Little information can be found on Meta CDN clusters except the [SOSP'13 paper](https://dl.acm.org/doi/10.1145/2517349.2522722). However, the object size distribution in the CDN dataset is significantly different from the photo CDN workload in the [SOSP'13 paper](https://dl.acm.org/doi/10.1145/2517349.2522722). I guess this is because most data served by Meta CDNs were photos in 2013, and most data served by Meta CDNs in 2023 were videos. 

Note that the meta CDN dataset might not be collected from the clusters in ISP, as the description says "Each cluster consists of 1000's of hosts". The number of servers is too large for ISP. 

<!-- The Cachelib team at Meta has been continously releasing new traces in the past few years.   -->


**My complaints**
* The Key-value traces do not have a timestamp field
* It is unclear which layer these caches are, from the skewness, I suspect there are other layers in front of these caches.



### Wikipedia (CDN traces)
Wikipedia has released three datasets in 2007, 2016 and 2019. The traces can be found [here](https://wikitech.wikimedia.org/wiki/Analytics/Data_Lake/Traffic/Caching).

#### 2007 dataset
The 2017 dataset can be downloaded from [here](http://www.wikibench.eu/?page_id=60). It 


#### 2016 dataset
The 2016 dataset contains two weeks' of requests to one server serving images (called upload). 
In total there are 2.8 billion requests (92 TB) for 37 million objects (5.4 TB).
The compulsory request miss ratio is around 1.3% and the compulsory byte miss ratio is around 5.8%.
The mean object size is 35 KB and the mean request size is 156 KB. The mean object frequency is around 76.
The original download link is invalid now. But I have previously downloaded the traces and I will include it in my dataset. 


#### 2019 dataset
The 2019 dataset can be downloaded from [here](https://analytics.wikimedia.org/published/datasets/caching/2019/). 
The 2019 dataset contains 21 days of traces from two servers, one serving webpages (called text), the other serve images (called upload). 

The upload trace contains 2.6 billion requests (101 TB) for 49 million objects (7.6 TB). 
The compulsory request miss ratio is 1.8% and the compulsory byte miss ratio is around 7.6%.
The mean object size is 35 KB and the mean request size is 156 KB. The mean object frequency is around 76.

The text trace contains 208 million requests (6.4 TB) for 18 million objects (0.4 TB).
The compulsory request miss ratio is around 8.8% and the compulsory byte miss ratio is around 6.2%.
The mean object size is 33 KB and the mean request size is 23 KB. The mean object frequency is around 11.


#### More information

Wikipedia uses Varnish as the CDN cache. Some metrics can be found at the [grafana dashboard](https://grafana.wikimedia.org/d/kHk7W6OZz/ats-cluster-view?orgId=1). Wikimedia provide more information about the [CDN architecture](https://wikitech.wikimedia.org/wiki/CDN) and [hardware](https://wikitech.wikimedia.org/wiki/CDN/Hardware). 

There are several blogs can be read to get a better understanding of the Wikimedia CDN architecture, [blog1](https://techblog.wikimedia.org/2020/10/14/wikimedias-cdn/), [blog2](https://techblog.wikimedia.org/2020/11/25/wikimedias-cdn-the-road-to-ats/), [blog3](https://techblog.wikimedia.org/2020/12/17/wikimedias-cdn-datacenter-switchover/).
* 
The metrics of 


one trace in  three traces in 2019, the details can be found [here](https://www.wikidata.org/wiki/Wikidata:Data_access#Wikimedia_CDN_traces).


### Alibaba (block traces)


### Tencent (block traces and CDN traces)


### Summary of the new datasets
<style>
    table {
        margin-left: auto;
        margin-right: auto;
    }
</style>



| Dataset collections | Collection time | Cache type | time span  (days) | Traces | Request  (million) | Request  (TB) | Object  (million) | Object  (TB) |
| :-----------------: | :-------------: | :--------: | :---------------: | :----: | :----------------: | :-----------: | :---------------: | :----------: |
|    Tencent Photo    |      2018       |   Object   |         8         |   2    |       5,650        |      141      |       1,038       |      24      |
|    WikiMedia CDN    |      2019       |   Object   |         7         |   3    |       2,863        |      200      |        56         |      13      |
|     Tencent CBS     |      2020       |   Block    |         8         |  4030  |       33,690       |     1091      |        551        |      66      |
|       Alibaba       |      2020       |   Block    |        30         |  652   |       19,676       |      664      |       1702        |     117      |
|       Twitter       |      2020       |     KV     |         7         |   54   |      195,441       |      106      |      10,650       |      6       |
|       Meta KV       |      2022       |     KV     |         5         |   1    |       1,644        |      958      |        82         |      76      |
|      Meta CDN       |      2023       |   Object   |         7         |   3    |        231         |     8,800     |        76         |     1,563    |

### How are these traces different from the old ones
As can be noticed, the new datasets 
* were all collected from distributed caches, including the three storage blockIO datasets.  
* are mostly multi-tenanted traces (except for Twitter and Wikipedia datasets)
* are mostly second-layer or third-layer caches (except for the Twitter dataset)
* either have much small object sizes (e.g., KV cache traces) or have  are much larger than the old ones.

## Curating a new dataset
Given the multiple new and huge datasets, it is non-trivial to evaluate on all of them. So in this section, I will describe how I selected one or a few traces from each open-source dataset to create a new representative dataset for cache eviction algorthm study. 
Considerations in curating the datasets
* The dataset should be representative of the modern workloads from different sources and different cache types.
* The dataset should not be too large and can be evaluated on a single machine.
* The dataset should not be too small and becomes unrelistic.   
* 


|   trace name | Cache type | time span  (days) | Request  (million) | Request  (TB) | Object  (million) | Object  (TB) | Typical miss ratio |
| -----------: | :--------: | :---------------: | :----------------: | :-----------: | :---------------: | :----------: | :----------------- |
| tencentPhoto |   Object   |         8         |       5,650        |      141      |       1,038       |      24      | [5% - 10%](https://www.usenix.org/conference/osdi20/presentation/yang)           |
|      wikiCDN |   Object   |         7         |       2,863        |      200      |        56         |      13      | [7%](https://grafana.wikimedia.org/d/b4tbXxn4z/cdn-global-hitrate?orgId=1), [3%-17%](https://techblog.wikimedia.org/2020/11/25/wikimedias-cdn-the-road-to-ats/) |
| tencentBlock |   Block    |         8         |       33,690       |     1091      |        551        |      66      | Typical miss ratio |
| alibabaBlock |   Block    |        30         |       19,676       |      664      |       1702        |     117      | Typical miss ratio |
|    twitterKV |     KV     |         7         |      195,441       |      106      |      10,650       |      6       | Typical miss ratio |
|       metaKV |     KV     |         5         |       8,220        |     4790      |        x82        |     x76      | Typical miss ratio |
|      metaCDN |   Object   |         7         |        231         |     8,800     |        x76        |    x1,563    | [40%](https://dl.acm.org/doi/10.1145/2517349.2522722)             |


### How are the traces selected
1. If a dataset contains multiple traces, I compared the traces in terms of request count, object count, and the LRU miss ratio curve. If the traces exhibit similar characteristics, I randomly picked one trace from the dataset.
Such dataset includes tencentPhoto, metaKV and metaCDN. 
2. For twitterKV traces, I picked the cluster52, cluster17, cluster18, cluster24, cluster44, cluster45, cluster29 traces recommended by the authors. 
3. For wikiCDN traces, since they are not large, I retained all three traces.
 wikiCDN, tencentBlock, alibabaBlock, and metaCDN.

### How the traces are processed
1. For most traces, I just extract the timestamp, object id (or block id), and the object size field. 
2. For some traces that do not have timestamp field, e.g., Wiki 2016 and MetaKV (202206) datasets, I assign a timestamp to each request by assuming a constant request rate. 
3. For datasets with more than 1 billion traces, e.g., TwitterKV, I used spatial sampling to reduce the number of objects in the trace. Note that, most of the traces were already down-sampled during collection time, e.g., collecting requests to one shard. I have verified that the down-sampled traces have similar characteristics as the original traces.
4. 

In my recent projects, 
However, the traces used in the literature are often outdated and do not reflect the modern workloads. In this post, I will discuss a new dataset for evaluating cache eviction algorithms. 





[^1]: For example, the traces in the [ARC paper](https://www.usenix.org/legacy/events/fast03/tech/full_papers/megiddo/megiddo.pdf) record requests after a page cache, which should not be used for evaluating algorithms for kernel page cache. 
[^2]: Disclaimer: I am one of the authors of this paper.


If you cite this blog post, please use the following bibtex entry:
```


```